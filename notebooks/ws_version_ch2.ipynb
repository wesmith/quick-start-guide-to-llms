{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "539b8dfe-6190-42ed-bd0b-0d2120d571f2",
   "metadata": {},
   "source": [
    "# ws_version_ch2.ipynb\n",
    "## WESmith 03/06/24\n",
    "\n",
    "## Working thru book's chapter 2 in a personalized way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ff09f-7879-44fc-8ab6-93c93c6f60fd",
   "metadata": {},
   "source": [
    "## TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8f9ac-d9eb-4e74-84c6-3f20a96acd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the tiktoken library\n",
    "import tiktoken\n",
    "\n",
    "# Initializing a tokenizer for the 'cl100k_base' model\n",
    "# This tokenizer is designed to work with the 'ada-002' embedding model\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Using the tokenizer to encode the text 'hey there'\n",
    "# The resulting output is a list of integers representing the encoded text\n",
    "# This is the input format required for embedding using the 'ada-002' model\n",
    "tokenizer.encode('hey there, you, and you')  # WS mods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924271a-0c8d-4fe4-aa4a-74e83353b303",
   "metadata": {},
   "source": [
    "## READING PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f2559e-ce0b-4530-b9e1-1cfb7eb1f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from tqdm import tqdm  # tqdm is a progress meter\n",
    "\n",
    "# Open the PDF file in read-binary mode\n",
    "author_file = '../data/pds2.pdf'                  # WS large, 16M\n",
    "ws_file     = '../data/journal.pone.0000404.pdf'  # WS much smaller, 561K\n",
    "\n",
    "pages = []  # WS\n",
    "\n",
    "with open(ws_file, 'rb') as file:                 # WS\n",
    "\n",
    "    # Create a PDF reader object\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    # Initialize an empty string to hold the text\n",
    "    #principles_of_ds = ''\n",
    "    file_txt = ''  # WS\n",
    "    \n",
    "    # Loop through each page in the PDF file\n",
    "    for page in tqdm(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        pages.append(text)   # WS\n",
    "        #principles_of_ds += '\\n\\n' + text[text.find(' ]')+2:]\n",
    "\n",
    "        # WS find() returns the number found; if ' ]' not found, -1 returned, and '\\n\\n' + text[2 - 1] is catenated:\n",
    "        # WS for my pdf, this misses the first char of the page: it, text[1] is added, not text[0]; \n",
    "        # WS this may work for author's pdf: it may have ] chars, my pdf doesn't have any ' ]' 'space-bracket' tokens\n",
    "        file_txt += '\\n\\n' + text[text.find(' ]')+2:]  # WS if ' ]' not found, -1 returned, \n",
    "\n",
    "# Print the final string containing all the text from the PDF file\n",
    "#principles_of_ds = principles_of_ds.strip()\n",
    "#file_txt = file_txt.strip()\n",
    "\n",
    "#print(len(principles_of_ds))\n",
    "print(len(file_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f38074-d142-4463-b5d0-609dd1dc0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = tokenizer.encode(pages[0])  # WS works\n",
    "pages[0][:70], dd[:10]  #WS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b9979b-76e8-4b43-8e87-488ef8f47fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee = tokenizer.encode(file_txt)  # WS\n",
    "file_txt[:50], ee[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204cde19-1260-4e03-832b-9fc717d3a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('Subspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602f5e5-e871-466d-b868-8ecb917c3ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca13ca4-b88c-4c8a-9a38-5653f9c72e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('ub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6efc0-7637-4e29-a395-1a03e83fd917",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('space')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82cf27-19b7-447d-ba5b-f06a70eff6ca",
   "metadata": {},
   "source": [
    "## HASHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838fec3c-2ca8-4c92-9fdc-98952ce08303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def my_hash(s):\n",
    "    # Return the MD5 hash of the input string as a hexadecimal string\n",
    "    return hashlib.md5(s.encode()).hexdigest()\n",
    "\n",
    "my_hash('I love to hash it')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caea119-bd9f-482c-8027-47cfe4dcad09",
   "metadata": {},
   "source": [
    "## OVERLAPPING CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d67e314-e781-4128-8f23-99457a763fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to split the text into chunks of a maximum number of tokens. Inspired by OpenAI\n",
    "def overlapping_chunks(text, max_tokens = 500, overlapping_factor = 5):\n",
    "    '''\n",
    "    max_tokens: tokens we want per chunk\n",
    "    overlapping_factor: number of sentences to start each chunk with that overlaps with the previous chunk\n",
    "    '''\n",
    "\n",
    "    # Split the text using punctuation\n",
    "    sentences = re.split(r'[.?!]', text)\n",
    "\n",
    "    # Get the number of tokens for each sentence\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    \n",
    "    chunks, tokens_so_far, chunk = [], 0, []\n",
    "\n",
    "    # Loop through the sentences and tokens joined together in a tuple\n",
    "    for sentence, token in zip(sentences, n_tokens):\n",
    "\n",
    "        # If the number of tokens so far plus the number of tokens in the current sentence is greater \n",
    "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
    "        # the chunk and tokens so far\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\". \".join(chunk) + \".\")\n",
    "            if overlapping_factor > 0:\n",
    "                chunk = chunk[-overlapping_factor:]\n",
    "                tokens_so_far = sum([len(tokenizer.encode(c)) for c in chunk])\n",
    "            else:\n",
    "                chunk = []\n",
    "                tokens_so_far = 0\n",
    "\n",
    "        # If the number of tokens in the current sentence is greater than the max number of \n",
    "        # tokens, go to the next sentence\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n",
    "        chunk.append(sentence)\n",
    "        tokens_so_far += token + 1\n",
    "    if chunk:\n",
    "        chunks.append(\". \".join(chunk) + \".\")\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7537589b-4cf5-49be-add0-2a1df6a0532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = overlapping_chunks(file_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6085f0-16a4-49a1-b5a4-ede20cb70bb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d4c38-7bd5-45d1-99c9-d1ec717975c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.encode(chunks[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8c571-981f-4d5f-89df-37ecd4ca83b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_1 = re.split(r'[.?!]', chunks[0])\n",
    "sentences_2 = re.split(r'[.?!]', chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f89b23c-8238-4d64-b881-ef1f8a3c4be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sentences_1[-7:]: print(k + '\\n')  # WS overlap is at the level of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048fdc50-6c4a-4e18-acd4-e3c806fe81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sentences_2[:7]: print(k + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a02f4-b8c1-4bee-82e3-82e1f8d45309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
