{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "539b8dfe-6190-42ed-bd0b-0d2120d571f2",
   "metadata": {},
   "source": [
    "# ws_version_ch2.ipynb\n",
    "## WESmith 03/06/24\n",
    "\n",
    "## Working thru book's chapter 2 in a personalized way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ff09f-7879-44fc-8ab6-93c93c6f60fd",
   "metadata": {},
   "source": [
    "## TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8f9ac-d9eb-4e74-84c6-3f20a96acd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the tiktoken library\n",
    "import tiktoken\n",
    "\n",
    "# Initializing a tokenizer for the 'cl100k_base' model\n",
    "# This tokenizer is designed to work with the 'ada-002' embedding model\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Using the tokenizer to encode the text 'hey there'\n",
    "# The resulting output is a list of integers representing the encoded text\n",
    "# This is the input format required for embedding using the 'ada-002' model\n",
    "tokenizer.encode('hey there, you, and you')  # WS mods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924271a-0c8d-4fe4-aa4a-74e83353b303",
   "metadata": {},
   "source": [
    "## READING PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f2559e-ce0b-4530-b9e1-1cfb7eb1f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from tqdm import tqdm  # tqdm is a progress meter\n",
    "\n",
    "# Open the PDF file in read-binary mode\n",
    "author_file = '../data/pds2.pdf'                  # WS large, 16M\n",
    "ws_file     = '../data/journal.pone.0000404.pdf'  # WS much smaller, 561K\n",
    "\n",
    "pages = []  # WS\n",
    "\n",
    "with open(ws_file, 'rb') as file:                 # WS\n",
    "\n",
    "    # Create a PDF reader object\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    # Initialize an empty string to hold the text\n",
    "    #principles_of_ds = ''\n",
    "    file_txt = ''  # WS\n",
    "    \n",
    "    # Loop through each page in the PDF file\n",
    "    for page in tqdm(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        pages.append(text)   # WS\n",
    "        #principles_of_ds += '\\n\\n' + text[text.find(' ]')+2:]\n",
    "\n",
    "        # WS find() returns the number found; if ' ]' not found, -1 returned, and '\\n\\n' + text[2 - 1] is catenated:\n",
    "        # WS for my pdf, this misses the first char of the page: it, text[1] is added, not text[0]; \n",
    "        # WS this may work for author's pdf: it may have ] chars, my pdf doesn't have any ' ]' 'space-bracket' tokens\n",
    "        file_txt += '\\n\\n' + text[text.find(' ]')+2:]  # WS if ' ]' not found, -1 returned, \n",
    "\n",
    "# Print the final string containing all the text from the PDF file\n",
    "#principles_of_ds = principles_of_ds.strip()\n",
    "#file_txt = file_txt.strip()\n",
    "\n",
    "#print(len(principles_of_ds))\n",
    "print(len(file_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f38074-d142-4463-b5d0-609dd1dc0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = tokenizer.encode(pages[0])  # WS works\n",
    "pages[0][:70], dd[:10]  #WS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b9979b-76e8-4b43-8e87-488ef8f47fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee = tokenizer.encode(file_txt)  # WS\n",
    "file_txt[:50], ee[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204cde19-1260-4e03-832b-9fc717d3a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('Subspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602f5e5-e871-466d-b868-8ecb917c3ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca13ca4-b88c-4c8a-9a38-5653f9c72e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('ub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6efc0-7637-4e29-a395-1a03e83fd917",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838fec3c-2ca8-4c92-9fdc-98952ce08303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
