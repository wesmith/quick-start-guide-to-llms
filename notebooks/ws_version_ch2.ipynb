{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "539b8dfe-6190-42ed-bd0b-0d2120d571f2",
   "metadata": {},
   "source": [
    "# ws_version_ch2.ipynb\n",
    "## WESmith 03/06/24\n",
    "\n",
    "## Working thru book's chapter 2 in a personalized way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ff09f-7879-44fc-8ab6-93c93c6f60fd",
   "metadata": {},
   "source": [
    "## TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8f9ac-d9eb-4e74-84c6-3f20a96acd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the tiktoken library\n",
    "import tiktoken\n",
    "\n",
    "# Initializing a tokenizer for the 'cl100k_base' model\n",
    "# This tokenizer is designed to work with the 'ada-002' embedding model\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Using the tokenizer to encode the text 'hey there'\n",
    "# The resulting output is a list of integers representing the encoded text\n",
    "# This is the input format required for embedding using the 'ada-002' model\n",
    "tokenizer.encode('hey there, you, and you')  # WS mods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924271a-0c8d-4fe4-aa4a-74e83353b303",
   "metadata": {},
   "source": [
    "## READING PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f2559e-ce0b-4530-b9e1-1cfb7eb1f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from tqdm import tqdm  # tqdm is a progress meter\n",
    "\n",
    "# Open the PDF file in read-binary mode\n",
    "author_file = '../data/pds2.pdf'                  # WS large, 16M\n",
    "ws_file     = '../data/journal.pone.0000404.pdf'  # WS much smaller, 561K\n",
    "\n",
    "pages = []  # WS\n",
    "\n",
    "with open(ws_file, 'rb') as file:                 # WS\n",
    "\n",
    "    # Create a PDF reader object\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    # Initialize an empty string to hold the text\n",
    "    #principles_of_ds = ''\n",
    "    file_txt = ''  # WS\n",
    "    \n",
    "    # Loop through each page in the PDF file\n",
    "    for page in tqdm(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        pages.append(text)   # WS\n",
    "        #principles_of_ds += '\\n\\n' + text[text.find(' ]')+2:]\n",
    "\n",
    "        # WS find() returns the number found; if ' ]' not found, -1 returned, and '\\n\\n' + text[2 - 1] is catenated:\n",
    "        # WS for my pdf, this misses the first char of the page: it, text[1] is added, not text[0]; \n",
    "        # WS this may work for author's pdf: it may have ] chars, my pdf doesn't have any ' ]' 'space-bracket' tokens\n",
    "        file_txt += '\\n\\n' + text[text.find(' ]')+2:]  # WS if ' ]' not found, -1 returned, \n",
    "\n",
    "# Print the final string containing all the text from the PDF file\n",
    "#principles_of_ds = principles_of_ds.strip()\n",
    "#file_txt = file_txt.strip()\n",
    "\n",
    "#print(len(principles_of_ds))\n",
    "print(len(file_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f38074-d142-4463-b5d0-609dd1dc0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = tokenizer.encode(pages[0])  # WS works\n",
    "pages[0][:70], dd[:10]  #WS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b9979b-76e8-4b43-8e87-488ef8f47fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee = tokenizer.encode(file_txt)  # WS\n",
    "file_txt[:50], ee[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204cde19-1260-4e03-832b-9fc717d3a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('Subspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602f5e5-e871-466d-b868-8ecb917c3ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca13ca4-b88c-4c8a-9a38-5653f9c72e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('ub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6efc0-7637-4e29-a395-1a03e83fd917",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('space')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82cf27-19b7-447d-ba5b-f06a70eff6ca",
   "metadata": {},
   "source": [
    "## HASHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838fec3c-2ca8-4c92-9fdc-98952ce08303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def my_hash(s):\n",
    "    # Return the MD5 hash of the input string as a hexadecimal string\n",
    "    return hashlib.md5(s.encode()).hexdigest()\n",
    "\n",
    "my_hash('I love to hash it')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caea119-bd9f-482c-8027-47cfe4dcad09",
   "metadata": {},
   "source": [
    "## OVERLAPPING CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d67e314-e781-4128-8f23-99457a763fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to split the text into chunks of a maximum number of tokens. Inspired by OpenAI\n",
    "def overlapping_chunks(text, max_tokens = 500, overlapping_factor = 5):\n",
    "    '''\n",
    "    max_tokens: tokens we want per chunk\n",
    "    overlapping_factor: number of sentences to start each chunk with that overlaps with the previous chunk\n",
    "    '''\n",
    "\n",
    "    # Split the text using punctuation\n",
    "    sentences = re.split(r'[.?!]', text)\n",
    "\n",
    "    # Get the number of tokens for each sentence\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    \n",
    "    chunks, tokens_so_far, chunk = [], 0, []\n",
    "\n",
    "    # Loop through the sentences and tokens joined together in a tuple\n",
    "    for sentence, token in zip(sentences, n_tokens):\n",
    "\n",
    "        # If the number of tokens so far plus the number of tokens in the current sentence is greater \n",
    "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
    "        # the chunk and tokens so far\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\". \".join(chunk) + \".\")\n",
    "            if overlapping_factor > 0:\n",
    "                chunk = chunk[-overlapping_factor:]\n",
    "                tokens_so_far = sum([len(tokenizer.encode(c)) for c in chunk])\n",
    "            else:\n",
    "                chunk = []\n",
    "                tokens_so_far = 0\n",
    "\n",
    "        # If the number of tokens in the current sentence is greater than the max number of \n",
    "        # tokens, go to the next sentence\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n",
    "        chunk.append(sentence)\n",
    "        tokens_so_far += token + 1\n",
    "    if chunk:\n",
    "        chunks.append(\". \".join(chunk) + \".\")\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7537589b-4cf5-49be-add0-2a1df6a0532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = overlapping_chunks(file_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6085f0-16a4-49a1-b5a4-ede20cb70bb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d4c38-7bd5-45d1-99c9-d1ec717975c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.encode(chunks[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8c571-981f-4d5f-89df-37ecd4ca83b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_1 = re.split(r'[.?!]', chunks[0])\n",
    "sentences_2 = re.split(r'[.?!]', chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f89b23c-8238-4d64-b881-ef1f8a3c4be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sentences_1[-7:]: print(k + '\\n')  # WS overlap is at the level of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048fdc50-6c4a-4e18-acd4-e3c806fe81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sentences_2[:7]: print(k + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e4b95-7941-436d-a3be-ccc158198807",
   "metadata": {},
   "source": [
    "## TEXT SOURCES FROM INTERNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a02f4-b8c1-4bee-82e3-82e1f8d45309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "# A textbook about insects\n",
    "text = urlopen('https://www.gutenberg.org/cache/epub/10834/pg10834.txt').read().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e936ee03-791b-4a28-98ef-696cec5c0c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Einstein = urlopen('https://www.gutenberg.org/cache/epub/30155/pg30155.txt').read().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de372b-98ed-4433-bbbb-047fd01a6a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "Einstein[1000:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d283d22-a1c9-45d3-b694-cfab812333e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1de928-9505-4ed0-8f26-0904e2a70ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = overlapping_chunks(text, overlapping_factor=0)\n",
    "avg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)\n",
    "print(f'non-overlapping chunking approach has {len(split)} documents with average length {avg_length:.1f} tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff4728c-7b7e-4607-9dfd-21474f101b15",
   "metadata": {},
   "source": [
    "## STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6024555d-e502-4f2b-a818-75a0350fdc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Counter and re libraries\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Find all occurrences of one or more spaces in 'principles_of_ds'\n",
    "matches = re.findall(r'[\\s]{1,}', text)\n",
    "\n",
    "# The 10 most frequent spaces that occur in the document\n",
    "most_common_spaces = Counter(matches).most_common(30)\n",
    "\n",
    "# Print the most common spaces and their frequencies\n",
    "print(most_common_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af88c229-f87c-4970-bfea-e3045ed85973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep documents of at least 100 characters split by a custom delimiter\n",
    "split = list(filter(lambda x: len(x) > 50, text.split('\\r\\n\\r\\n')))  # WS using \\r\\n\\r\\n for this text sample\n",
    "\n",
    "avg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)\n",
    "print(f'custom delimiter approach has {len(split)} documents with average length {avg_length:.1f} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc3de9-9bea-4662-9bbc-77350707fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "split[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52da4a51-3e2d-4902-a556-db698550d393",
   "metadata": {},
   "source": [
    "## SENTENCE EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af10ce-bab7-40b7-9fda-f01c1a4c3736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://pypi.org/project/sentence-transformers/\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe45e6c3-9407-476a-8e00-822f24152409",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"This framework generates embeddings for each input sentence\",\n",
    "    \"Sentences are passed as a list of string.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",]\n",
    "sentence_embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e50e8-a396-47f9-8502-e1b4ecf658c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding[0:20])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff792cbe-373a-48cf-90d3-a82f33e721c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98b38de-7515-4901-a654-af373c2bc779",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188c07b-50a3-41f4-8412-1eb142bef5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e3a3b-8763-4c5f-a2bb-f535c5ce49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_encoded = model.encode(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efdcb59-6082-4f84-89a6-082397b7cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3fbebb-607b-4915-bb8f-49cb41f69226",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_encoded[3].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae4566-3714-4dd2-a972-e49f5ea4931c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
