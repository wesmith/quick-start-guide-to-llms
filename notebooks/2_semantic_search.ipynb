{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa447fa7-6f85-4c6c-8bf0-d5f5ad9f9e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook has been updated to use the latest openai package version! At the time, 1.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Chapter 2: Launching an Application with Proprietary Models \n",
    "    Overview of Proprietary Models\n",
    "    Introduction to OpenAI + Embeddings / GPT3 / ChatGPT\n",
    "    Introduction to Vector Databases\n",
    "    Building a Neural/Semantic Information Retrieval System with Vector Databases, BERT & GPT3\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12182ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0289c54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pinecone_key = os.environ.get('PINECONE_API_KEY')\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "INDEX_NAME = 'semantic-search'\n",
    "NAMESPACE = 'default'\n",
    "ENGINE = 'text-embedding-ada-002'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9befee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(api_key=pinecone_key, environment=\"us-west1-gcp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf993a3-6d51-49f4-8968-60f654d6202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to get lists of embeddings from the OpenAI API\n",
    "def get_embeddings(texts, engine=ENGINE):\n",
    "    response = client.embeddings.create(\n",
    "        input=texts,\n",
    "        model=engine\n",
    "    )\n",
    "    \n",
    "    return [d.embedding for d in list(response.data)]\n",
    "\n",
    "def get_embedding(text):\n",
    "    return get_embeddings([text])[0]\n",
    "    \n",
    "len(get_embedding('hi')), len(get_embeddings(['hi', 'hello']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea70672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not INDEX_NAME in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        INDEX_NAME,  # The name of the index\n",
    "        dimension=1536,  # The dimensionality of the vectors\n",
    "        metric='cosine',  # The similarity metric to use when searching the index\n",
    "        pod_type=\"p1\"  # The type of Pinecone pod\n",
    "    )\n",
    "\n",
    "# Store the index as a variable\n",
    "index = pinecone.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8088892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6103d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2fdfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_hash(s):\n",
    "    # Return the MD5 hash of the input string as a hexadecimal string\n",
    "    return hashlib.md5(s.encode()).hexdigest()\n",
    "\n",
    "my_hash('I love to hash it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd86f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_pinecone(texts, engine=ENGINE):\n",
    "    # Get the current UTC date and time\n",
    "    now = datetime.utcnow()\n",
    "    \n",
    "    # Generate vector embeddings for each string in the input list, using the specified engine\n",
    "    embeddings = get_embeddings(texts, engine=engine)\n",
    "    \n",
    "    # Create tuples of (hash, embedding, metadata) for each input string and its corresponding vector embedding\n",
    "    # The my_hash() function is used to generate a unique hash for each string, and the datetime.utcnow() function is used to generate the current UTC date and time\n",
    "    return [\n",
    "        (\n",
    "            my_hash(text),  # A unique ID for each string, generated using the my_hash() function\n",
    "            embedding,  # The vector embedding of the string\n",
    "            dict(text=text, date_uploaded=now)  # A dictionary of metadata, including the original text and the current UTC date and time\n",
    "        ) \n",
    "        for text, embedding in zip(texts, embeddings)  # Iterate over each input string and its corresponding vector embedding\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065c037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prepare_for_pinecone(texts)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_id, embedding, metadata = prepare_for_pinecone(texts)[0]\n",
    "\n",
    "print('ID:  ',_id, '\\nLEN: ', len(embedding), '\\nMETA:', metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49debd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf47aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_texts_to_pinecone(texts, namespace=NAMESPACE, batch_size=None, show_progress_bar=False):\n",
    "    # Call the prepare_for_pinecone function to prepare the input texts for indexing\n",
    "    total_upserted = 0\n",
    "    if not batch_size:\n",
    "        batch_size = len(texts)\n",
    "\n",
    "    _range = range(0, len(texts), batch_size)\n",
    "    for i in tqdm(_range) if show_progress_bar else _range:\n",
    "        batch = texts[i: i + batch_size]\n",
    "        prepared_texts = prepare_for_pinecone(batch)\n",
    "\n",
    "        # Use the upsert() method of the index object to upload the prepared texts to Pinecone\n",
    "        total_upserted += index.upsert(\n",
    "            prepared_texts,\n",
    "            namespace=namespace\n",
    "        )['upserted_count']\n",
    "\n",
    "    return total_upserted\n",
    "\n",
    "\n",
    "# Call the upload_texts_to_pinecone() function with the input texts\n",
    "upload_texts_to_pinecone(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ed7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_from_pinecone(query, top_k=3):\n",
    "    # get embedding from THE SAME embedder as the documents\n",
    "    query_embedding = get_embedding(query, engine=ENGINE)\n",
    "\n",
    "    return index.query(\n",
    "      vector=query_embedding,\n",
    "      top_k=top_k,\n",
    "      namespace=NAMESPACE,\n",
    "      include_metadata=True   # gets the metadata (dates, text, etc)\n",
    "    ).get('matches')\n",
    "\n",
    "query_from_pinecone('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a0871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def delete_texts_from_pinecone(texts, namespace=NAMESPACE):\n",
    "    # Compute the hash (id) for each text\n",
    "    hashes = [hashlib.md5(text.encode()).hexdigest() for text in texts]\n",
    "    \n",
    "    # The ids parameter is used to specify the list of IDs (hashes) to delete\n",
    "    return index.delete(ids=hashes, namespace=namespace)\n",
    "\n",
    "# delete our text\n",
    "delete_texts_from_pinecone(texts)\n",
    "\n",
    "# test that the index is empty\n",
    "query_from_pinecone('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1bac4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72729603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the tiktoken library\n",
    "import tiktoken\n",
    "\n",
    "# Initializing a tokenizer for the 'cl100k_base' model\n",
    "# This tokenizer is designed to work with the 'ada-002' embedding model\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Using the tokenizer to encode the text 'hey there'\n",
    "# The resulting output is a list of integers representing the encoded text\n",
    "# This is the input format required for embedding using the 'ada-002' model\n",
    "tokenizer.encode('hey there')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc147d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f34d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the text into chunks of a maximum number of tokens. Inspired by OpenAI\n",
    "def overlapping_chunks(text, max_tokens = 500, overlapping_factor = 5):\n",
    "    '''\n",
    "    max_tokens: tokens we want per chunk\n",
    "    overlapping_factor: number of sentences to start each chunk with that overlaps with the previous chunk\n",
    "    '''\n",
    "\n",
    "    # Split the text using punctuation\n",
    "    sentences = re.split(r'[.?!]', text)\n",
    "\n",
    "    # Get the number of tokens for each sentence\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    \n",
    "    chunks, tokens_so_far, chunk = [], 0, []\n",
    "\n",
    "    # Loop through the sentences and tokens joined together in a tuple\n",
    "    for sentence, token in zip(sentences, n_tokens):\n",
    "\n",
    "        # If the number of tokens so far plus the number of tokens in the current sentence is greater \n",
    "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
    "        # the chunk and tokens so far\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\". \".join(chunk) + \".\")\n",
    "            if overlapping_factor > 0:\n",
    "                chunk = chunk[-overlapping_factor:]\n",
    "                tokens_so_far = sum([len(tokenizer.encode(c)) for c in chunk])\n",
    "            else:\n",
    "                chunk = []\n",
    "                tokens_so_far = 0\n",
    "\n",
    "        # If the number of tokens in the current sentence is greater than the max number of \n",
    "        # tokens, go to the next sentence\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n",
    "        chunk.append(sentence)\n",
    "        tokens_so_far += token + 1\n",
    "    if chunk:\n",
    "        chunks.append(\". \".join(chunk) + \".\")\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3164e0ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Open the PDF file in read-binary mode\n",
    "with open('../data/pds2.pdf', 'rb') as file:\n",
    "\n",
    "    # Create a PDF reader object\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    # Initialize an empty string to hold the text\n",
    "    principles_of_ds = ''\n",
    "    # Loop through each page in the PDF file\n",
    "    for page in tqdm(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        principles_of_ds += '\\n\\n' + text[text.find(' ]')+2:]\n",
    "\n",
    "# Print the final string containing all the text from the PDF file\n",
    "principles_of_ds = principles_of_ds.strip()\n",
    "\n",
    "print(len(principles_of_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e01a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "#\n",
    "\n",
    "# A textbook about insects\n",
    "text = urlopen('https://www.gutenberg.org/cache/epub/10834/pg10834.txt').read().decode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdac6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = overlapping_chunks(principles_of_ds, overlapping_factor=0)\n",
    "avg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)\n",
    "print(f'non-overlapping chunking approach has {len(split)} documents with average length {avg_length:.1f} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = overlapping_chunks(principles_of_ds)\n",
    "avg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)\n",
    "print(f'overlapping chunking approach has {len(split)} documents with average length {avg_length:.1f} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1381cc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758cb1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc7dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Counter and re libraries\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Find all occurrences of one or more spaces in 'principles_of_ds'\n",
    "matches = re.findall(r'[\\s]{1,}', principles_of_ds)\n",
    "\n",
    "# The 10 most frequent spaces that occur in the document\n",
    "most_common_spaces = Counter(matches).most_common(10)\n",
    "\n",
    "# Print the most common spaces and their frequencies\n",
    "print(most_common_spaces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ccc253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep documents of at least 100 characters split by a custom delimiter\n",
    "split = list(filter(lambda x: len(x) > 50, principles_of_ds.split('\\n\\n')))\n",
    "\n",
    "avg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)\n",
    "print(f'custom delimiter approach has {len(split)} documents with average length {avg_length:.1f} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480fe3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings = None\n",
    "for s in tqdm(range(0, len(split), 100)):\n",
    "    if embeddings is None:\n",
    "        embeddings = np.array(get_embeddings(split[s:s+100], engine=ENGINE))\n",
    "    else:\n",
    "        embeddings = np.vstack([embeddings, np.array(get_embeddings(split[s:s+100], engine=ENGINE))])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe8fd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Assume you have a list of text embeddings called `embeddings`\n",
    "# First, compute the cosine similarity matrix between all pairs of embeddings\n",
    "cosine_sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Instantiate the AgglomerativeClustering model\n",
    "agg_clustering = AgglomerativeClustering(\n",
    "    n_clusters=None,         # the algorithm will determine the optimal number of clusters based on the data\n",
    "    distance_threshold=0.1,  # clusters will be formed until all pairwise distances between clusters are greater than 0.1\n",
    "    affinity='precomputed',  # we are providing a precomputed distance matrix (1 - similarity matrix) as input\n",
    "    linkage='complete'       # form clusters by iteratively merging the smallest clusters based on the maximum distance between their components\n",
    ")\n",
    "\n",
    "# Fit the model to the cosine distance matrix (1 - similarity matrix)\n",
    "agg_clustering.fit(1 - cosine_sim_matrix)\n",
    "\n",
    "# Get the cluster labels for each embedding\n",
    "cluster_labels = agg_clustering.labels_\n",
    "\n",
    "# Print the number of embeddings in each cluster\n",
    "unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f'Cluster {label}: {count} embeddings')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0727319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_documents = []\n",
    "for _label, count in zip(unique_labels, counts):\n",
    "    pruned_documents.append('\\n\\n'.join([text for text, label in zip(split, cluster_labels) if label == _label]))\n",
    "\n",
    "    \n",
    "avg_length = sum([len(tokenizer.encode(t)) for t in pruned_documents]) / len(pruned_documents)\n",
    "print(f'Our pruning approach has {len(pruned_documents)} documents with average length {avg_length:.1f} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f80b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pruned_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bafae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ebbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a46ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3364dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_texts_to_pinecone(pruned_documents, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511c945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cc8bdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = 'How do z scores work?'\n",
    "\n",
    "results_from_pinecone = query_from_pinecone(query, top_k=5)\n",
    "\n",
    "for result_from_pinecone in results_from_pinecone:\n",
    "    print(f\"{result_from_pinecone['id']}\\t{result_from_pinecone['score']:.2f}\\t{result_from_pinecone['metadata']['text'][:50]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074fab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This example computes the score between a query and all possible\n",
    "sentences in a corpus using a Cross-Encoder for semantic textual similarity (STS).\n",
    "It output then the most similar sentences for the given query.\n",
    "\"\"\"\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "# Pre-trained cross encoder\n",
    "cross_encoder = CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384-v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b8e942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414fc2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_pinecone(query, top_k=3, re_rank=False, verbose=True):\n",
    "\n",
    "    results_from_pinecone = query_from_pinecone(query, top_k=top_k)\n",
    "    if not results_from_pinecone:\n",
    "        return []\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Query:\", query)\n",
    "    \n",
    "    \n",
    "    final_results = []\n",
    "\n",
    "    if re_rank:\n",
    "        if verbose:\n",
    "            print('Document ID (Hash)\\t\\tRetrieval Score\\tCE Score\\tText')\n",
    "\n",
    "        sentence_combinations = [[query, result_from_pinecone['metadata']['text']] for result_from_pinecone in results_from_pinecone]\n",
    "\n",
    "        # Compute the similarity scores for these combinations\n",
    "        similarity_scores = cross_encoder.predict(sentence_combinations, activation_fct=nn.Sigmoid())\n",
    "\n",
    "        # Sort the scores in decreasing order\n",
    "        sim_scores_argsort = reversed(np.argsort(similarity_scores))\n",
    "\n",
    "        # Print the scores\n",
    "        for idx in sim_scores_argsort:\n",
    "            result_from_pinecone = results_from_pinecone[idx]\n",
    "            final_results.append(result_from_pinecone)\n",
    "            if verbose:\n",
    "                print(f\"{result_from_pinecone['id']}\\t{result_from_pinecone['score']:.2f}\\t{similarity_scores[idx]:.2f}\\t{result_from_pinecone['metadata']['text'][:50]}\")\n",
    "        return final_results\n",
    "\n",
    "    if verbose:\n",
    "        print('Document ID (Hash)\\t\\tRetrieval Score\\tText')\n",
    "    for result_from_pinecone in results_from_pinecone:\n",
    "        final_results.append(result_from_pinecone)\n",
    "        if verbose:\n",
    "            print(f\"{result_from_pinecone['id']}\\t{result_from_pinecone['score']:.2f}\\t{result_from_pinecone['metadata']['text'][:50]}\")\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89bcc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = get_results_from_pinecone(query, top_k=3, re_rank=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c9922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = get_results_from_pinecone(query, top_k=10, re_rank=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_texts_from_pinecone(pruned_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c716a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79e1caf7",
   "metadata": {},
   "source": [
    "# BoolQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e28784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"boolq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6773457",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221343a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in tqdm(range(0, len(dataset['validation']), 256)):\n",
    "    data_sample = dataset['validation'][idx:idx + 256]\n",
    "\n",
    "    passages = data_sample['passage']\n",
    "\n",
    "    upload_texts_to_pinecone(passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802a9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "query = sample(dataset['validation']['question'], 1)[0]\n",
    "print(query)\n",
    "final_results = get_results_from_pinecone(query, top_k=3, re_rank=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e27222",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_to_hash = {data['question']: my_hash(data['passage']) for data in dataset['validation']}\n",
    "\n",
    "q_to_hash[query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ec53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# super_glue_metric = load('super_glue', 'boolq')  # just accuracy\n",
    "\n",
    "# Let's test the performance re-ranking against 1000 of our validation datapoints\n",
    "# Note we could not use Pinecone here to speed things up\n",
    "#  but it's also a good time to test latency of the pipeline with Pinecone\n",
    "val_sample = dataset['validation']#[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a3a147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfea2ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Note we will keep top_k the same so latency from Pinecone is consistent\n",
    "#  and the only major time difference will be in the re-ranking\n",
    "\n",
    "for question in tqdm(val_sample['question']):\n",
    "    retrieved_hash = get_results_from_pinecone(question, top_k=1, re_rank=False, verbose=False)[0]['id']\n",
    "    correct_hash = q_to_hash[question]\n",
    "    predictions.append(retrieved_hash == correct_hash)\n",
    "    \n",
    "accuracy = sum(predictions)/len(predictions)\n",
    "\n",
    "print(f'Accuracy without re-ranking: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98413a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Note we will keep top_k the same so latency from Pinecone is consistent\n",
    "#  and the only major time difference will be in the re-ranking\n",
    "\n",
    "for question in tqdm(val_sample['question']):\n",
    "    retrieved_hash = get_results_from_pinecone(question, top_k=3, re_rank=True, verbose=False)[0]['id']\n",
    "    correct_hash = q_to_hash[question]\n",
    "    predictions.append(retrieved_hash == correct_hash)\n",
    "    \n",
    "accuracy = sum(predictions)/len(predictions)\n",
    "\n",
    "print(f'Accuracy with re-ranking: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5acf707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the time differences between with and without re-ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfebd73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ca98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ranking(query, cross_encoder, top_k=3):\n",
    "    results_from_pinecone = query_from_pinecone(query, top_k=top_k)\n",
    "    sentence_combinations = [[query, result_from_pinecone['metadata']['text']] for result_from_pinecone in results_from_pinecone]\n",
    "    similarity_scores = cross_encoder.predict(sentence_combinations)\n",
    "    sim_scores_argsort = list(reversed(np.argsort(similarity_scores)))\n",
    "    re_ranked_final_result = results_from_pinecone[sim_scores_argsort[0]]\n",
    "    return results_from_pinecone[0]['id'], re_ranked_final_result['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4010029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying another pre-trained cross encoder\n",
    "# sentence-transformers/multi-qa-mpnet-base-cos-v1\n",
    "newer_cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f34ac11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104305e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "print_every = 50\n",
    "predictions = []\n",
    "for question in tqdm(val_sample['question']):\n",
    "    retrieved_hash, reranked_hash = eval_ranking(question, newer_cross_encoder, top_k=3)\n",
    "    correct_hash = q_to_hash[question]\n",
    "    predictions.append((retrieved_hash == correct_hash, reranked_hash == correct_hash))\n",
    "    i += 1\n",
    "    if i % print_every == 0:\n",
    "        print(f'Step {i}')\n",
    "        raw_accuracy = sum([p[0] for p in predictions])/len(predictions)\n",
    "        reranked_accuracy = sum([p[1] for p in predictions])/len(predictions)\n",
    "\n",
    "        print(f'Accuracy without re-ranking: {raw_accuracy}')\n",
    "        print(f'Accuracy with re-ranking: {reranked_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502d5e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_accuracy = sum([p[0] for p in predictions])/len(predictions)\n",
    "reranked_accuracy = sum([p[1] for p in predictions])/len(predictions)\n",
    "\n",
    "print(f'Using cross-encoder: {newer_cross_encoder.config._name_or_path}')\n",
    "print(f'Accuracy without re-ranking: {raw_accuracy}')\n",
    "print(f'Accuracy with re-ranking: {reranked_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea30153c",
   "metadata": {},
   "source": [
    "# Fine-tuning re-ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9918c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/ms_marco/train_cross-encoder_scratch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508edaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e68f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf1607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "from random import shuffle\n",
    "\n",
    "shuffled_training_passages = dataset['train']['passage'].copy()\n",
    "shuffle(shuffled_training_passages)\n",
    "\n",
    "\n",
    "train_samples = [\n",
    "  InputExample(texts=[d['question'], d['passage']], label=1) for d in dataset['train']\n",
    "]\n",
    "\n",
    "# add some negative examples\n",
    "train_samples += [\n",
    "  InputExample(texts=[d['question'], shuffled_training_passages[i]], label=0) for i, d in enumerate(dataset['train'])\n",
    "]\n",
    "\n",
    "shuffle(train_samples)\n",
    "\n",
    "# running the risk of overfitting on my data but maybe I want that. \n",
    "#  Combined with sufficient input and output validation, we can make a viable product with a model overfit to my data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7826e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b58d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2', num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7eaa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede1b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(train_samples[0].texts, activation_fct=nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2bc9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator, CEBinaryClassificationEvaluator\n",
    "import math\n",
    "import torch\n",
    "from random import sample\n",
    "\n",
    "logger.setLevel(logging.DEBUG)  # just to get some logs\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "model_save_path = './fine_tuned_ir_cross_encoder'\n",
    "\n",
    "# train_samples = sample(train_samples, 1000)\n",
    "\n",
    "# int(len(train_samples)*.8)\n",
    "train_dataloader = DataLoader(train_samples[:int(len(train_samples)*.8)], shuffle=True, batch_size=32)\n",
    "\n",
    "# An evaluator for training performance\n",
    "evaluator = CEBinaryClassificationEvaluator.from_input_examples(train_samples[-int(len(train_samples)*.8):], name='test')\n",
    "\n",
    "# Rule of thumb for warmup steps\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)  # 10% of train data for warm-up\n",
    "print(f\"Warmup-steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de4dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ##### Load model and eval on test set\n",
    "# print(evaluator(model))\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    loss_fct=losses.nn.CrossEntropyLoss(),\n",
    "    activation_fct=nn.Sigmoid(),\n",
    "    evaluator=evaluator,\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=model_save_path,\n",
    "    use_amp=True\n",
    ")\n",
    "\n",
    "# ##### Load model and eval on test set\n",
    "# print(evaluator(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca59b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the more fine tuned version on open source as well to match??\n",
    "# depends if it does better here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f747537",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned = CrossEncoder(model_save_path)\n",
    "\n",
    "print(finetuned.predict(['hello', 'hi'], activation_fct=nn.Sigmoid()))\n",
    "print(finetuned.predict(['hello', 'hi'], activation_fct=nn.Identity()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c1f756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf736e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trying our fine-tuned cross encoder\n",
    "logger.setLevel(logging.CRITICAL)  # just to suppress some logs\n",
    "from tqdm import tqdm\n",
    "\n",
    "i = 0\n",
    "print_every = 50\n",
    "predictions = []\n",
    "for question in tqdm(val_sample['question']):\n",
    "    retrieved_hash, reranked_hash = eval_ranking(question, finetuned, top_k=3)\n",
    "    correct_hash = q_to_hash[question]\n",
    "    predictions.append((retrieved_hash == correct_hash, reranked_hash == correct_hash))\n",
    "    i += 1\n",
    "    if i % print_every == 0:\n",
    "        print(f'Step {i}')\n",
    "        raw_accuracy = sum([p[0] for p in predictions])/len(predictions)\n",
    "        reranked_accuracy = sum([p[1] for p in predictions])/len(predictions)\n",
    "\n",
    "        print(f'Accuracy without re-ranking: {raw_accuracy}')\n",
    "        print(f'Accuracy with re-ranking: {reranked_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb46e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-ranking got slightly better after 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6e0a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_accuracy = sum([p[0] for p in predictions])/len(predictions)\n",
    "reranked_accuracy = sum([p[1] for p in predictions])/len(predictions)\n",
    "\n",
    "print(f'Using cross-encoder: {finetuned.config._name_or_path}')\n",
    "print(f'Accuracy without re-ranking: {raw_accuracy}')\n",
    "print(f'Accuracy with re-ranking: {reranked_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92875bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b414c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2535cbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinecone.delete_index(INDEX_NAME)  # delete the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f14809e",
   "metadata": {},
   "source": [
    "# OPEN SOURCE ALTERNATIVE TO EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99138055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\n",
    "\n",
    "docs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n",
    "\n",
    "doc_emb = model.encode(docs, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "doc_emb.shape#  == ('2, 768')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1140c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be37c48b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Encode query and documents\n",
    "docs = dataset['validation']['passage']\n",
    "doc_emb = model.encode(docs, batch_size=32, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0afee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "query = sample(dataset['validation']['question'], 1)[0]\n",
    "print(query)\n",
    "final_results = get_results_from_pinecone(query, top_k=3, re_rank=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c71fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_emb = model.encode(query)\n",
    "\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
    "\n",
    "#Combine docs & scores\n",
    "doc_score_pairs = list(zip(docs, scores))\n",
    "\n",
    "#Sort by decreasing score\n",
    "doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Output passages & scores\n",
    "for doc, score in doc_score_pairs[:3]:\n",
    "    print(score, doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8533d9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93280be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.CRITICAL)  # just to suppress some logs\n",
    "\n",
    "\n",
    "def eval_ranking_open_source(query, cross_encoder, top_k=3):\n",
    "    query_emb = np.array(get_embedding(query, engine=ENGINE))\n",
    "\n",
    "    #Compute dot score between query and all document embeddings\n",
    "    scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
    "\n",
    "    #Combine docs & scores\n",
    "    doc_score_pairs = list(zip(docs, scores))\n",
    "\n",
    "    #Sort by decreasing score\n",
    "    doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    retrieved_hash = my_hash(doc_score_pairs[0][0])\n",
    "    if cross_encoder:\n",
    "        sentence_combinations = [[query, doc_score_pair[0]] for doc_score_pair in doc_score_pairs]\n",
    "        similarity_scores = cross_encoder.predict(sentence_combinations)\n",
    "        sim_scores_argsort = list(reversed(np.argsort(similarity_scores)))\n",
    "        reranked_hash = my_hash(doc_score_pairs[sim_scores_argsort[0]][0])\n",
    "    else:\n",
    "        reranked_hash = None\n",
    "    return retrieved_hash, reranked_hash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f198cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_ranking_open_source(query, finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faf0c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "i = 0\n",
    "print_every = 50\n",
    "predictions = []\n",
    "for question in tqdm(val_sample['question']):\n",
    "    retrieved_hash, reranked_hash = eval_ranking_open_source(question, finetuned, top_k=3)\n",
    "    correct_hash = q_to_hash[question]\n",
    "    predictions.append((retrieved_hash == correct_hash, reranked_hash == correct_hash))\n",
    "    i += 1\n",
    "    if i % print_every == 0:\n",
    "        print(f'Step {i}')\n",
    "        raw_accuracy = sum([p[0] for p in predictions])/len(predictions)\n",
    "        reranked_accuracy = sum([p[1] for p in predictions])/len(predictions)\n",
    "\n",
    "        print(f'Accuracy without re-ranking: {raw_accuracy}')\n",
    "        print(f'Accuracy with re-ranking: {reranked_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaeb9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_accuracy = sum([p[0] for p in predictions])/len(predictions)\n",
    "reranked_accuracy = sum([p[1] for p in predictions])/len(predictions)\n",
    "\n",
    "print(f'Using cross-encoder: {finetuned.config._name_or_path}')\n",
    "print(f'Accuracy without re-ranking: {raw_accuracy}')\n",
    "print(f'Accuracy with re-ranking: {reranked_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca7d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
